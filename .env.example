# Server Configuration
HOST=0.0.0.0
PORT=7000
APP_PORT=7000  # Alias for PORT, used in monitoring configs

# Model Configuration
MODEL_NAME=meta-llama/Llama-3.2-3B-Instruct
DEFAULT_MAX_TOKENS=256  # Default when max_tokens is not specified in requests
MAX_ALLOWED_TOKENS=4096  # Maximum allowed value for max_tokens
TEMPERATURE=0.7
TRUST_REMOTE_CODE=true  # Allow models with custom code (e.g., DeepSeek)

# Model Loading Optimization
LOW_CPU_MEM_USAGE=true  # Load models with minimal CPU memory
TORCH_DTYPE=auto        # Use optimal dtype (bfloat16/float16)

# Remote API Options (for very large models)
DEEPSEEK_API_KEY=your_api_key_here  # Use DeepSeek's official API
OPENAI_API_KEY=your_api_key_here    # Fallback to OpenAI
USE_REMOTE_FOR_LARGE_MODELS=false   # Auto-fallback for models >7B

# Monitoring Ports
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000

# Prometheus Configuration
PROMETHEUS_SCRAPE_INTERVAL=10s
PROMETHEUS_EVAL_INTERVAL=10s
PROMETHEUS_SCRAPE_TIMEOUT=5s

# Backend Selection (auto, mlx, vllm, cpu)
INFERENCE_BACKEND=auto

# Cache Configuration
MAX_MODEL_CACHE_SIZE=3
MODEL_CACHE_TTL=3600

# Logging
LOG_LEVEL=INFO

# Workers (production mode only)
WORKERS=1

# API Keys Database
API_KEYS_DB_PATH=data/api_keys.db

# Rate Limiting
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_WINDOW=60
# Application Settings
APP_NAME="Activate LLM Inference Server"
APP_VERSION="1.0.0"
DEBUG=false

# Server Configuration
HOST=0.0.0.0
PORT=8000
WORKERS=1

# Model Configuration
MODEL_PATH=mlx-community/Llama-3.2-1B-Instruct-bf16
MODEL_CACHE_DIR=~/.cache/mlx-models
MAX_MODEL_CACHE_SIZE=5

# Inference Settings
DEFAULT_TEMPERATURE=0.7
DEFAULT_MAX_TOKENS=256
MAX_ALLOWED_TOKENS=4096
TIMEOUT_SECONDS=300

# Memory Management
MAX_MEMORY_GB=8.0
GPU_MEMORY_FRACTION=0.8

# Security (comma-separated API keys)
API_KEYS=your-api-key-1,your-api-key-2
API_KEY_HEADER=X-API-Key

# CORS
ENABLE_CORS=true
CORS_ORIGINS=*

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS=60
RATE_LIMIT_PERIOD=60

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json
LOG_FILE=

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=9090
EOF < /dev/null
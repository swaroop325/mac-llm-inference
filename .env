# Application Settings
APP_NAME="LLM Inference Server"
APP_VERSION="1.0.0"
DEBUG=false

# Server Configuration
HOST=0.0.0.0
PORT=7000

# Model Configuration
MODEL_PATH=meta-llama/Llama-3.2-1B-Instruct
MODEL_CACHE_DIR=~/.cache/llm-models
MAX_MODEL_CACHE_SIZE=5

# Inference Settings
DEFAULT_TEMPERATURE=0
DEFAULT_MAX_TOKENS=256
MAX_ALLOWED_TOKENS=4096
TIMEOUT_SECONDS=300
MODEL_LOAD_TIMEOUT_SECONDS=600

# Memory Management
MAX_MEMORY_GB=8.0
GPU_MEMORY_FRACTION=0.8

# Security (comma-separated API keys)
# API_KEYS=
API_KEY_HEADER=X-API-Key

# CORS (leave simple - no lists)
ENABLE_CORS=true

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS=60
RATE_LIMIT_PERIOD=60

# Logging
LOG_LEVEL=info
LOG_FORMAT=json
LOG_FILE=logs/llm_server.log

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=9090

# HuggingFace Tokenizers
TOKENIZERS_PARALLELISM=false

# Inference Backend Selection
# Options: mlx (Apple Silicon), vllm (NVIDIA/AMD GPU), cpu (CPU fallback), auto (detect automatically)
#INFERENCE_BACKEND=vllm
INFERENCE_BACKEND=mlx